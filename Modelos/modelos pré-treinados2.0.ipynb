{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac73d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classes reservadas:\n",
    "# Classes do aluno Messyas\n",
    "# 1 - Class ID:546 Class Name: electric guitar   \n",
    "# 2 - Class ID:508 Class Name: computer keyboard, keypad\n",
    "# 3 - Class ID:614 Class Name: kimono\n",
    "# Classes do aluno Jorge\n",
    "# 4 - Class ID:366 Class Name: gorilla, Gorilla gorilla   \n",
    "# 5 - Class ID:501 Class Name: cloak      \n",
    "# 6 - Class ID:365 Class Name: orangutan, orang, orangutang, Pongo pygmaeus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "17158a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "340fc394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo inception_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8a6e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "23990e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = InceptionV3(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52a7ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Messy/teti_img'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd35e884",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'electric guitar.jpg')\n",
    "img = image.load_img(image_path, target_size=(299, 299))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c0353bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_3\n",
      "1 conv2d_188\n",
      "2 batch_normalization_188\n",
      "3 activation_188\n",
      "4 conv2d_189\n",
      "5 batch_normalization_189\n",
      "6 activation_189\n",
      "7 conv2d_190\n",
      "8 batch_normalization_190\n",
      "9 activation_190\n",
      "10 max_pooling2d_8\n",
      "11 conv2d_191\n",
      "12 batch_normalization_191\n",
      "13 activation_191\n",
      "14 conv2d_192\n",
      "15 batch_normalization_192\n",
      "16 activation_192\n",
      "17 max_pooling2d_9\n",
      "18 conv2d_196\n",
      "19 batch_normalization_196\n",
      "20 activation_196\n",
      "21 conv2d_194\n",
      "22 conv2d_197\n",
      "23 batch_normalization_194\n",
      "24 batch_normalization_197\n",
      "25 activation_194\n",
      "26 activation_197\n",
      "27 average_pooling2d_18\n",
      "28 conv2d_193\n",
      "29 conv2d_195\n",
      "30 conv2d_198\n",
      "31 conv2d_199\n",
      "32 batch_normalization_193\n",
      "33 batch_normalization_195\n",
      "34 batch_normalization_198\n",
      "35 batch_normalization_199\n",
      "36 activation_193\n",
      "37 activation_195\n",
      "38 activation_198\n",
      "39 activation_199\n",
      "40 mixed0\n",
      "41 conv2d_203\n",
      "42 batch_normalization_203\n",
      "43 activation_203\n",
      "44 conv2d_201\n",
      "45 conv2d_204\n",
      "46 batch_normalization_201\n",
      "47 batch_normalization_204\n",
      "48 activation_201\n",
      "49 activation_204\n",
      "50 average_pooling2d_19\n",
      "51 conv2d_200\n",
      "52 conv2d_202\n",
      "53 conv2d_205\n",
      "54 conv2d_206\n",
      "55 batch_normalization_200\n",
      "56 batch_normalization_202\n",
      "57 batch_normalization_205\n",
      "58 batch_normalization_206\n",
      "59 activation_200\n",
      "60 activation_202\n",
      "61 activation_205\n",
      "62 activation_206\n",
      "63 mixed1\n",
      "64 conv2d_210\n",
      "65 batch_normalization_210\n",
      "66 activation_210\n",
      "67 conv2d_208\n",
      "68 conv2d_211\n",
      "69 batch_normalization_208\n",
      "70 batch_normalization_211\n",
      "71 activation_208\n",
      "72 activation_211\n",
      "73 average_pooling2d_20\n",
      "74 conv2d_207\n",
      "75 conv2d_209\n",
      "76 conv2d_212\n",
      "77 conv2d_213\n",
      "78 batch_normalization_207\n",
      "79 batch_normalization_209\n",
      "80 batch_normalization_212\n",
      "81 batch_normalization_213\n",
      "82 activation_207\n",
      "83 activation_209\n",
      "84 activation_212\n",
      "85 activation_213\n",
      "86 mixed2\n",
      "87 conv2d_215\n",
      "88 batch_normalization_215\n",
      "89 activation_215\n",
      "90 conv2d_216\n",
      "91 batch_normalization_216\n",
      "92 activation_216\n",
      "93 conv2d_214\n",
      "94 conv2d_217\n",
      "95 batch_normalization_214\n",
      "96 batch_normalization_217\n",
      "97 activation_214\n",
      "98 activation_217\n",
      "99 max_pooling2d_10\n",
      "100 mixed3\n",
      "101 conv2d_222\n",
      "102 batch_normalization_222\n",
      "103 activation_222\n",
      "104 conv2d_223\n",
      "105 batch_normalization_223\n",
      "106 activation_223\n",
      "107 conv2d_219\n",
      "108 conv2d_224\n",
      "109 batch_normalization_219\n",
      "110 batch_normalization_224\n",
      "111 activation_219\n",
      "112 activation_224\n",
      "113 conv2d_220\n",
      "114 conv2d_225\n",
      "115 batch_normalization_220\n",
      "116 batch_normalization_225\n",
      "117 activation_220\n",
      "118 activation_225\n",
      "119 average_pooling2d_21\n",
      "120 conv2d_218\n",
      "121 conv2d_221\n",
      "122 conv2d_226\n",
      "123 conv2d_227\n",
      "124 batch_normalization_218\n",
      "125 batch_normalization_221\n",
      "126 batch_normalization_226\n",
      "127 batch_normalization_227\n",
      "128 activation_218\n",
      "129 activation_221\n",
      "130 activation_226\n",
      "131 activation_227\n",
      "132 mixed4\n",
      "133 conv2d_232\n",
      "134 batch_normalization_232\n",
      "135 activation_232\n",
      "136 conv2d_233\n",
      "137 batch_normalization_233\n",
      "138 activation_233\n",
      "139 conv2d_229\n",
      "140 conv2d_234\n",
      "141 batch_normalization_229\n",
      "142 batch_normalization_234\n",
      "143 activation_229\n",
      "144 activation_234\n",
      "145 conv2d_230\n",
      "146 conv2d_235\n",
      "147 batch_normalization_230\n",
      "148 batch_normalization_235\n",
      "149 activation_230\n",
      "150 activation_235\n",
      "151 average_pooling2d_22\n",
      "152 conv2d_228\n",
      "153 conv2d_231\n",
      "154 conv2d_236\n",
      "155 conv2d_237\n",
      "156 batch_normalization_228\n",
      "157 batch_normalization_231\n",
      "158 batch_normalization_236\n",
      "159 batch_normalization_237\n",
      "160 activation_228\n",
      "161 activation_231\n",
      "162 activation_236\n",
      "163 activation_237\n",
      "164 mixed5\n",
      "165 conv2d_242\n",
      "166 batch_normalization_242\n",
      "167 activation_242\n",
      "168 conv2d_243\n",
      "169 batch_normalization_243\n",
      "170 activation_243\n",
      "171 conv2d_239\n",
      "172 conv2d_244\n",
      "173 batch_normalization_239\n",
      "174 batch_normalization_244\n",
      "175 activation_239\n",
      "176 activation_244\n",
      "177 conv2d_240\n",
      "178 conv2d_245\n",
      "179 batch_normalization_240\n",
      "180 batch_normalization_245\n",
      "181 activation_240\n",
      "182 activation_245\n",
      "183 average_pooling2d_23\n",
      "184 conv2d_238\n",
      "185 conv2d_241\n",
      "186 conv2d_246\n",
      "187 conv2d_247\n",
      "188 batch_normalization_238\n",
      "189 batch_normalization_241\n",
      "190 batch_normalization_246\n",
      "191 batch_normalization_247\n",
      "192 activation_238\n",
      "193 activation_241\n",
      "194 activation_246\n",
      "195 activation_247\n",
      "196 mixed6\n",
      "197 conv2d_252\n",
      "198 batch_normalization_252\n",
      "199 activation_252\n",
      "200 conv2d_253\n",
      "201 batch_normalization_253\n",
      "202 activation_253\n",
      "203 conv2d_249\n",
      "204 conv2d_254\n",
      "205 batch_normalization_249\n",
      "206 batch_normalization_254\n",
      "207 activation_249\n",
      "208 activation_254\n",
      "209 conv2d_250\n",
      "210 conv2d_255\n",
      "211 batch_normalization_250\n",
      "212 batch_normalization_255\n",
      "213 activation_250\n",
      "214 activation_255\n",
      "215 average_pooling2d_24\n",
      "216 conv2d_248\n",
      "217 conv2d_251\n",
      "218 conv2d_256\n",
      "219 conv2d_257\n",
      "220 batch_normalization_248\n",
      "221 batch_normalization_251\n",
      "222 batch_normalization_256\n",
      "223 batch_normalization_257\n",
      "224 activation_248\n",
      "225 activation_251\n",
      "226 activation_256\n",
      "227 activation_257\n",
      "228 mixed7\n",
      "229 conv2d_260\n",
      "230 batch_normalization_260\n",
      "231 activation_260\n",
      "232 conv2d_261\n",
      "233 batch_normalization_261\n",
      "234 activation_261\n",
      "235 conv2d_258\n",
      "236 conv2d_262\n",
      "237 batch_normalization_258\n",
      "238 batch_normalization_262\n",
      "239 activation_258\n",
      "240 activation_262\n",
      "241 conv2d_259\n",
      "242 conv2d_263\n",
      "243 batch_normalization_259\n",
      "244 batch_normalization_263\n",
      "245 activation_259\n",
      "246 activation_263\n",
      "247 max_pooling2d_11\n",
      "248 mixed8\n",
      "249 conv2d_268\n",
      "250 batch_normalization_268\n",
      "251 activation_268\n",
      "252 conv2d_265\n",
      "253 conv2d_269\n",
      "254 batch_normalization_265\n",
      "255 batch_normalization_269\n",
      "256 activation_265\n",
      "257 activation_269\n",
      "258 conv2d_266\n",
      "259 conv2d_267\n",
      "260 conv2d_270\n",
      "261 conv2d_271\n",
      "262 average_pooling2d_25\n",
      "263 conv2d_264\n",
      "264 batch_normalization_266\n",
      "265 batch_normalization_267\n",
      "266 batch_normalization_270\n",
      "267 batch_normalization_271\n",
      "268 conv2d_272\n",
      "269 batch_normalization_264\n",
      "270 activation_266\n",
      "271 activation_267\n",
      "272 activation_270\n",
      "273 activation_271\n",
      "274 batch_normalization_272\n",
      "275 activation_264\n",
      "276 mixed9_0\n",
      "277 concatenate_4\n",
      "278 activation_272\n",
      "279 mixed9\n",
      "280 conv2d_277\n",
      "281 batch_normalization_277\n",
      "282 activation_277\n",
      "283 conv2d_274\n",
      "284 conv2d_278\n",
      "285 batch_normalization_274\n",
      "286 batch_normalization_278\n",
      "287 activation_274\n",
      "288 activation_278\n",
      "289 conv2d_275\n",
      "290 conv2d_276\n",
      "291 conv2d_279\n",
      "292 conv2d_280\n",
      "293 average_pooling2d_26\n",
      "294 conv2d_273\n",
      "295 batch_normalization_275\n",
      "296 batch_normalization_276\n",
      "297 batch_normalization_279\n",
      "298 batch_normalization_280\n",
      "299 conv2d_281\n",
      "300 batch_normalization_273\n",
      "301 activation_275\n",
      "302 activation_276\n",
      "303 activation_279\n",
      "304 activation_280\n",
      "305 batch_normalization_281\n",
      "306 activation_273\n",
      "307 mixed9_1\n",
      "308 concatenate_5\n",
      "309 activation_281\n",
      "310 mixed10\n",
      "311 avg_pool\n",
      "312 predictions\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(modelo.layers):\n",
    "    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9310a81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 762ms/step\n",
      "Predicao: [('n03272010', 'electric_guitar', 0.99152714), ('n02676566', 'acoustic_guitar', 0.0016314896), ('n03929660', 'pick', 0.0005411711)]\n"
     ]
    }
   ],
   "source": [
    "preds = modelo.predict(x)\n",
    "print('Predicao:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ec6321f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'electric guitars.jpg')\n",
    "img = image.load_img(image_path, target_size=(299, 299))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8c506d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n",
      "Predicao: [('n03272010', 'electric_guitar', 0.96806115), ('n02676566', 'acoustic_guitar', 0.0011765038), ('n04296562', 'stage', 0.0007642876)]\n"
     ]
    }
   ],
   "source": [
    "preds = modelo.predict(x)\n",
    "print('Predicao:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fff772fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classe keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "82557f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'Keyboard.jpeg')\n",
    "img = image.load_img(image_path, target_size=(299, 299))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9c8cd8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n",
      "Predicao: [('n04127249', 'safety_pin', 0.44831055), ('n04557648', 'water_bottle', 0.031217186), ('n04026417', 'purse', 0.02139536)]\n"
     ]
    }
   ],
   "source": [
    "preds = modelo.predict(x)\n",
    "print('Predicao:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "09975509",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'keyboard2.jpg')\n",
    "img = image.load_img(image_path, target_size=(299, 299))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc2231b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n",
      "Predicao: [('n04264628', 'space_bar', 0.53068006), ('n03085013', 'computer_keyboard', 0.4198169), ('n04505470', 'typewriter_keyboard', 0.011488374)]\n"
     ]
    }
   ],
   "source": [
    "preds = modelo.predict(x)\n",
    "print('Predicao:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "75eb6d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLASSE KIMONO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b20aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'kimono.jpg')\n",
    "img = image.load_img(image_path, target_size=(299, 299))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "92b7a842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step\n",
      "Predicao: [('n04350905', 'suit', 0.410082), ('n03594734', 'jean', 0.09380575), ('n04370456', 'sweatshirt', 0.02696536)]\n"
     ]
    }
   ],
   "source": [
    "preds = modelo.predict(x)\n",
    "print('Predicao:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10950d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'kimonogirl.jpeg')\n",
    "img = image.load_img(image_path, target_size=(299, 299))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "42a620f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n",
      "Predicao: [('n04507155', 'umbrella', 0.29300734), ('n04259630', 'sombrero', 0.26609546), ('n04458633', 'totem_pole', 0.031122841)]\n"
     ]
    }
   ],
   "source": [
    "preds = modelo.predict(x)\n",
    "print('Predicao:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fe81eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLASSE GORILA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "01614607",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'silverback.jpg')\n",
    "img = image.load_img(image_path, target_size=(299, 299))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "11b87b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step\n",
      "Predicao: [('n02480855', 'gorilla', 0.74834675), ('n02483362', 'gibbon', 0.09853502), ('n02484975', 'guenon', 0.017605389)]\n"
     ]
    }
   ],
   "source": [
    "preds = modelo.predict(x)\n",
    "print('Predicao:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "976ba5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'gorila.jpg')\n",
    "img = image.load_img(image_path, target_size=(299, 299))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e92ca6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n",
      "Predicao: [('n02480855', 'gorilla', 0.8882147), ('n02483708', 'siamang', 0.003304979), ('n02483362', 'gibbon', 0.0026252721)]\n"
     ]
    }
   ],
   "source": [
    "preds = modelo.predict(x)\n",
    "print('Predicao:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d59edc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classe cloak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "023478bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'cloak.png')\n",
    "img = image.load_img(image_path, target_size=(299, 299))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "df9cdf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step\n",
      "Predicao: [('n03045698', 'cloak', 0.9548963), ('n03404251', 'fur_coat', 0.011497834), ('n04370456', 'sweatshirt', 0.006539882)]\n"
     ]
    }
   ],
   "source": [
    "preds = modelo.predict(x)\n",
    "print('Predicao:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d5994719",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'cloak2.jpg')\n",
    "img = image.load_img(image_path, target_size=(299, 299))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c178b783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step\n",
      "Predicao: [('n03045698', 'cloak', 0.78516006), ('n04532106', 'vestment', 0.07441328), ('n03980874', 'poncho', 0.008946522)]\n"
     ]
    }
   ],
   "source": [
    "preds = modelo.predict(x)\n",
    "print('Predicao:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ed13ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classe oranguntango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "44778ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'orangutan.jpg')\n",
    "img = image.load_img(image_path, target_size=(299, 299))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8548726b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n",
      "Predicao: [('n02480495', 'orangutan', 0.8753405), ('n02483708', 'siamang', 0.0096349), ('n02483362', 'gibbon', 0.0040472224)]\n"
     ]
    }
   ],
   "source": [
    "preds = modelo.predict(x)\n",
    "print('Predicao:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2da2c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'Orang.jpg')\n",
    "img = image.load_img(image_path, target_size=(299, 299))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7ecb1219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n",
      "Predicao: [('n02480495', 'orangutan', 0.86155385), ('n02483708', 'siamang', 0.006037505), ('n02481823', 'chimpanzee', 0.0014838548)]\n"
     ]
    }
   ],
   "source": [
    "preds = modelo.predict(x)\n",
    "print('Predicao:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3e0b5",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Guitarras</title>\n",
    "</head>\n",
    "<body>\n",
    "    <img src=\"https://cdn.discordapp.com/attachments/1122973016666673224/1125451990323957790/guitarims.jpeg\" style=\"width:700;height:560px\">\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a2e2efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparacao de imagens classe eletric guitar em diversos cenários de luminosidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dd5a1995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imagem com boa luminosidade \n",
    "image_path = os.path.join(path, 'al.jpeg')\n",
    "img = image.load_img(image_path, target_size=(299, 299))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aeba4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n",
      "Predicao: [('n03272010', 'electric_guitar', 0.9534084), ('n02676566', 'acoustic_guitar', 0.008329137), ('n03929660', 'pick', 0.0019043887)]\n"
     ]
    }
   ],
   "source": [
    "preds = modelo.predict(x)\n",
    "print('Predicao:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9ccf7632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imagem com luminosidade mediana\n",
    "image_path = os.path.join(path, 'ml.jpeg')\n",
    "img = image.load_img(image_path, target_size=(299, 299))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d055abf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n",
      "Predicao: [('n03272010', 'electric_guitar', 0.980023), ('n02676566', 'acoustic_guitar', 0.0020509306), ('n03929660', 'pick', 0.0011636657)]\n"
     ]
    }
   ],
   "source": [
    "preds = modelo.predict(x)\n",
    "print('Predicao:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "00e7b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imagem com baixa luminosidade \n",
    "image_path = os.path.join(path, 'bl.jpeg')\n",
    "img = image.load_img(image_path, target_size=(299, 299))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "91b31322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step\n",
      "Predicao: [('n03272010', 'electric_guitar', 0.9845296), ('n02676566', 'acoustic_guitar', 0.0027862738), ('n03929660', 'pick', 0.0009857025)]\n"
     ]
    }
   ],
   "source": [
    "preds = modelo.predict(x)\n",
    "print('Predicao:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4735332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Modelo inception_v3 com pytorch\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "666d983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "999e1473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o modelo pré-treinado Inception_v3\n",
    "model = models.inception_v3(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a8438a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inception3(\n",
       "  (Conv2d_1a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2b_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Conv2d_3b_1x1): BasicConv2d(\n",
       "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_4a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Mixed_5b): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5c): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5d): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6a): InceptionB(\n",
       "    (branch3x3): BasicConv2d(\n",
       "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6b): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6c): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6d): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6e): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (AuxLogits): InceptionAux(\n",
       "    (conv0): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv1): BasicConv2d(\n",
       "      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       "  (Mixed_7a): InceptionD(\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7b): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7c): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configura o modelo para modo de validação \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d7f144f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # Tamanho de entrada do Inception_v3 é 299x299\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c9c125b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Messy/teti_img/'\n",
    "img = Image.open(path + 'al.jpeg') #iluminacao alta\n",
    "img_tensor = transform(img)\n",
    "img_tensor = img_tensor.unsqueeze(0) # Adiciona batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fb58b7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = model(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2dc790d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_k_probs, top_k_indices = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "58580a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electric guitar: 0.9536\n",
      "acoustic guitar: 0.0052\n",
      "pick: 0.0019\n"
     ]
    }
   ],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Mostra as predições com probabilidade\n",
    "for i in range(len(top_k_probs[0])):\n",
    "    print(f\"{labels[top_k_indices[0][i]]}: {top_k_probs[0][i].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee11b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variacao com iluminacao mediana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "711d671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(path + 'ml.jpeg')\n",
    "img_tensor = transform(img)\n",
    "img_tensor = img_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1ac36119",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = model(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "66037fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_k_probs, top_k_indices = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f3081dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electric guitar: 0.9780\n",
      "acoustic guitar: 0.0016\n",
      "pick: 0.0015\n"
     ]
    }
   ],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "for i in range(len(top_k_probs[0])):\n",
    "    print(f\"{labels[top_k_indices[0][i]]}: {top_k_probs[0][i].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d8d3b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(path + 'bl.jpeg')#Baixa Luminosidade\n",
    "img_tensor = transform(img)\n",
    "img_tensor = img_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "32385f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = model(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1ac1f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_k_probs, top_k_indices = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e9cb2d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electric guitar: 0.9862\n",
      "acoustic guitar: 0.0019\n",
      "pick: 0.0014\n"
     ]
    }
   ],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "for i in range(len(top_k_probs[0])):\n",
    "    print(f\"{labels[top_k_indices[0][i]]}: {top_k_probs[0][i].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a301717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classe keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c349883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(path + 'Keyboard.jpeg')\n",
    "img_tensor = transform(img)\n",
    "img_tensor = img_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "eb685850",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = model(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "74ac5853",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_k_probs, top_k_indices = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "69353b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "safety pin: 0.3050\n",
      "modem: 0.0485\n",
      "remote control: 0.0284\n"
     ]
    }
   ],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "for i in range(len(top_k_probs[0])):\n",
    "    print(f\"{labels[top_k_indices[0][i]]}: {top_k_probs[0][i].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e8877e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(path + 'keyboard2.jpg')\n",
    "img_tensor = transform(img)\n",
    "img_tensor = img_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7b86589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = model(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "92b1e53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_k_probs, top_k_indices = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2fbbd0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer keyboard: 0.6115\n",
      "space bar: 0.2779\n",
      "mouse: 0.0234\n"
     ]
    }
   ],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "for i in range(len(top_k_probs[0])):\n",
    "    print(f\"{labels[top_k_indices[0][i]]}: {top_k_probs[0][i].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e4eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classe kimono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1696eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(path + 'kimono.jpg')\n",
    "img_tensor = transform(img)\n",
    "img_tensor = img_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9e9237a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = model(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b3682904",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_k_probs, top_k_indices = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "87b0175b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suit: 0.2469\n",
      "jean: 0.2143\n",
      "bulletproof vest: 0.0353\n"
     ]
    }
   ],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "for i in range(len(top_k_probs[0])):\n",
    "    print(f\"{labels[top_k_indices[0][i]]}: {top_k_probs[0][i].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d0bd4fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(path + 'kimonogirl.jpeg')\n",
    "img_tensor = transform(img)\n",
    "img_tensor = img_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "708a7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = model(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0014ba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_k_probs, top_k_indices = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f1b25207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sombrero: 0.3301\n",
      "umbrella: 0.1296\n",
      "cowboy hat: 0.0340\n"
     ]
    }
   ],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "for i in range(len(top_k_probs[0])):\n",
    "    print(f\"{labels[top_k_indices[0][i]]}: {top_k_probs[0][i].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df0f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classe gorila "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7c5f54ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(path + 'gorila.jpg')\n",
    "img_tensor = transform(img)\n",
    "img_tensor = img_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1c901856",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = model(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "846efb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_k_probs, top_k_indices = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1f65b49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gorilla: 0.8889\n",
      "guenon: 0.0025\n",
      "siamang: 0.0021\n"
     ]
    }
   ],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "for i in range(len(top_k_probs[0])):\n",
    "    print(f\"{labels[top_k_indices[0][i]]}: {top_k_probs[0][i].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4e5850c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(path + 'silverback.jpg')\n",
    "img_tensor = transform(img)\n",
    "img_tensor = img_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "acb2cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = model(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0b95d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_k_probs, top_k_indices = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2e97ada0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gorilla: 0.6159\n",
      "gibbon: 0.1775\n",
      "siamang: 0.0231\n"
     ]
    }
   ],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "for i in range(len(top_k_probs[0])):\n",
    "    print(f\"{labels[top_k_indices[0][i]]}: {top_k_probs[0][i].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db46440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cloak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7b75a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(path + 'cloak2.jpg')\n",
    "img_tensor = transform(img)\n",
    "img_tensor = img_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "8b1ced83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = model(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "1180dc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_k_probs, top_k_indices = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f2600c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloak: 0.9491\n",
      "vestment: 0.0194\n",
      "fur coat: 0.0016\n"
     ]
    }
   ],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "for i in range(len(top_k_probs[0])):\n",
    "    print(f\"{labels[top_k_indices[0][i]]}: {top_k_probs[0][i].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097bce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classe orangutang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b9e91e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(path + 'orangutan.jpg')\n",
    "img_tensor = transform(img)\n",
    "img_tensor = img_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "336e49f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = model(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "f3fb1011",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_k_probs, top_k_indices = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "cf957d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orangutan: 0.8281\n",
      "siamang: 0.0168\n",
      "gibbon: 0.0081\n"
     ]
    }
   ],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "for i in range(len(top_k_probs[0])):\n",
    "    print(f\"{labels[top_k_indices[0][i]]}: {top_k_probs[0][i].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ce80db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(path + 'Orang.jpg')\n",
    "img_tensor = transform(img)\n",
    "img_tensor = img_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "24eb2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = model(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "432b73c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_k_probs, top_k_indices = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "cabfc88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orangutan: 0.9153\n",
      "siamang: 0.0041\n",
      "chimpanzee: 0.0018\n"
     ]
    }
   ],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "for i in range(len(top_k_probs[0])):\n",
    "    print(f\"{labels[top_k_indices[0][i]]}: {top_k_probs[0][i].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4eadffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ef16ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo VGG 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "83ebd54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "8cf7960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizando keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "6e821f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "75b47084",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG19(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b7d1f2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Messy/teti_img'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "10b9e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'electric guitar.jpg')\n",
    "img = image.load_img(image_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "05b8114d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_4\n",
      "1 block1_conv1\n",
      "2 block1_conv2\n",
      "3 block1_pool\n",
      "4 block2_conv1\n",
      "5 block2_conv2\n",
      "6 block2_pool\n",
      "7 block3_conv1\n",
      "8 block3_conv2\n",
      "9 block3_conv3\n",
      "10 block3_conv4\n",
      "11 block3_pool\n",
      "12 block4_conv1\n",
      "13 block4_conv2\n",
      "14 block4_conv3\n",
      "15 block4_conv4\n",
      "16 block4_pool\n",
      "17 block5_conv1\n",
      "18 block5_conv2\n",
      "19 block5_conv3\n",
      "20 block5_conv4\n",
      "21 block5_pool\n",
      "22 flatten\n",
      "23 fc1\n",
      "24 fc2\n",
      "25 predictions\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "5c87720b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 205ms/step\n",
      "Prediction: [('n03272010', 'electric_guitar', 0.9791686), ('n02676566', 'acoustic_guitar', 0.018188171), ('n02787622', 'banjo', 0.0012512361)]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x)\n",
    "print('Prediction:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "585ca11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'electric guitars.jpg')\n",
    "img = image.load_img(image_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "7b85f6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 111ms/step\n",
      "Prediction: [('n03272010', 'electric_guitar', 0.98829144), ('n02676566', 'acoustic_guitar', 0.008954442), ('n04536866', 'violin', 0.00038365342)]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x)\n",
    "print('Prediction:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "0db88e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classe keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ecddf9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'Keyboard.jpeg')\n",
    "img = image.load_img(image_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "d60ef270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 114ms/step\n",
      "Prediction: [('n04026417', 'purse', 0.39259034), ('n03908618', 'pencil_box', 0.15957703), ('n04548362', 'wallet', 0.12053021)]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x)\n",
    "print('Prediction:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "45d92ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'keyboard2.jpg')\n",
    "img = image.load_img(image_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e0537607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 112ms/step\n",
      "Prediction: [('n03085013', 'computer_keyboard', 0.4287816), ('n04264628', 'space_bar', 0.36714795), ('n04505470', 'typewriter_keyboard', 0.058514256)]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x)\n",
    "print('Prediction:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e594b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classe kimono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c3c3da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'kimono.jpg')\n",
    "img = image.load_img(image_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f38e6de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 112ms/step\n",
      "Prediction: [('n03594734', 'jean', 0.0939866), ('n04228054', 'ski', 0.08035329), ('n04208210', 'shovel', 0.062201735)]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x)\n",
    "print('Prediction:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "78a77465",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'kimonogirl.jpeg')\n",
    "img = image.load_img(image_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "67a8d05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 112ms/step\n",
      "Prediction: [('n03617480', 'kimono', 0.8575138), ('n04507155', 'umbrella', 0.053683672), ('n03124043', 'cowboy_boot', 0.028406069)]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x)\n",
    "print('Prediction:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "939ba93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classe Gorila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "39951820",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'silverback.jpg')\n",
    "img = image.load_img(image_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "5db3497d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 112ms/step\n",
      "Prediction: [('n02480855', 'gorilla', 0.9769245), ('n02483708', 'siamang', 0.0106356805), ('n02483362', 'gibbon', 0.010282729)]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x)\n",
    "print('Prediction:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c9b31b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'gorila.jpg')\n",
    "img = image.load_img(image_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5f9c00ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 113ms/step\n",
      "Prediction: [('n02480855', 'gorilla', 0.99991405), ('n02481823', 'chimpanzee', 5.8368867e-05), ('n02480495', 'orangutan', 1.6007734e-05)]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x)\n",
    "print('Prediction:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a124eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classe orangutango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "385b5326",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'orangutan.jpg')\n",
    "img = image.load_img(image_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "64bae7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 113ms/step\n",
      "Prediction: [('n02480495', 'orangutan', 0.98870623), ('n02483708', 'siamang', 0.0051251687), ('n02493509', 'titi', 0.002473586)]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x)\n",
    "print('Prediction:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "a10c3444",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'Orang.jpg')\n",
    "img = image.load_img(image_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "06206e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 113ms/step\n",
      "Prediction: [('n02480495', 'orangutan', 0.9985279), ('n02492660', 'howler_monkey', 0.0012929116), ('n02493509', 'titi', 7.176367e-05)]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x)\n",
    "print('Prediction:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f834803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparacao com a imagem da guitarra em diferentes situacoes de luminosidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce03fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alta luminosidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "b032d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'al.jpeg')\n",
    "img = image.load_img(image_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "1b4abd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 124ms/step\n",
      "Prediction: [('n03272010', 'electric_guitar', 0.74668306), ('n02676566', 'acoustic_guitar', 0.23836286), ('n02787622', 'banjo', 0.00598169)]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x)\n",
    "print('Prediction:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f4ab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#luminosidade mediana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "a52990f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'ml.jpeg')\n",
    "img = image.load_img(image_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "41c05aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 116ms/step\n",
      "Prediction: [('n03272010', 'electric_guitar', 0.9303139), ('n02676566', 'acoustic_guitar', 0.06806682), ('n02787622', 'banjo', 0.0011064636)]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x)\n",
    "print('Prediction:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#luminosidade baixa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "b56aea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path, 'bl.jpeg')\n",
    "img = image.load_img(image_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "e2f85ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 116ms/step\n",
      "Prediction: [('n03272010', 'electric_guitar', 0.95217407), ('n02676566', 'acoustic_guitar', 0.04504313), ('n02787622', 'banjo', 0.0013666247)]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x)\n",
    "print('Prediction:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b87c6b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#vgg19 com pytorch \n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "1fbc328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchvision.models import vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "7c62d529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\takan\\anaconda3\\envs\\messy\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "modelo = vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "26bfb74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diretorio_imagens = 'C:/Messy/teti_img/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "b612e6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizacao\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "718f61ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = Image.open(diretorio_imagens + 'gorila.jpg')\n",
    "imagem = transform(imagem)\n",
    "x = imagem.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "763d5636",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = modelo(x)\n",
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_probs, top_idxs = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "a19b8f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "a8d9d577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe prevista: gorilla, Probabilidade: 0.9947178959846497\n",
      "Classe prevista: guenon, Probabilidade: 0.0031056220177561045\n",
      "Classe prevista: chimpanzee, Probabilidade: 0.0007130576996132731\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(top_idxs[0])):\n",
    "    print(f\"Classe prevista: {labels[top_idxs[0][i]]}, Probabilidade: {top_probs[0][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "205a7904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classe guitar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "307c7814",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = Image.open(diretorio_imagens + 'electric guitars.jpg')\n",
    "imagem = transform(imagem)\n",
    "x = imagem.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "b90e3223",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = modelo(x)\n",
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_probs, top_idxs = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "4dff4f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "236a5c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe prevista: electric guitar, Probabilidade: 0.9946088790893555\n",
      "Classe prevista: acoustic guitar, Probabilidade: 0.00341374846175313\n",
      "Classe prevista: pick, Probabilidade: 0.0008298454922623932\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(top_idxs[0])):\n",
    "    print(f\"Classe prevista: {labels[top_idxs[0][i]]}, Probabilidade: {top_probs[0][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "d79c3f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = Image.open(diretorio_imagens + 'Keyboard.jpeg')\n",
    "imagem = transform(imagem)\n",
    "x = imagem.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "cda6af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = modelo(x)\n",
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_probs, top_idxs = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "6f3051bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "f3cd3ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe prevista: pencil box, Probabilidade: 0.15997004508972168\n",
      "Classe prevista: rubber eraser, Probabilidade: 0.13216061890125275\n",
      "Classe prevista: cellular telephone, Probabilidade: 0.07594175636768341\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(top_idxs[0])):\n",
    "    print(f\"Classe prevista: {labels[top_idxs[0][i]]}, Probabilidade: {top_probs[0][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "55f3cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classe cloak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "53022eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = Image.open(diretorio_imagens + 'cloak2.jpg')\n",
    "imagem = transform(imagem)\n",
    "x = imagem.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "c5e6453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = modelo(x)\n",
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_probs, top_idxs = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "4fd6342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "c0cfaf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe prevista: cloak, Probabilidade: 0.9904830455780029\n",
      "Classe prevista: vestment, Probabilidade: 0.0065399473533034325\n",
      "Classe prevista: overskirt, Probabilidade: 0.001146620837971568\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(top_idxs[0])):\n",
    "    print(f\"Classe prevista: {labels[top_idxs[0][i]]}, Probabilidade: {top_probs[0][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guitarra em varias situações de liminosidade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "23aa9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = Image.open(diretorio_imagens + 'al.jpeg') #alta luminosidade\n",
    "imagem = transform(imagem)\n",
    "x = imagem.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "e38c07b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = modelo(x)\n",
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_probs, top_idxs = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "3ca4e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "a280d203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe prevista: electric guitar, Probabilidade: 0.9893345236778259\n",
      "Classe prevista: acoustic guitar, Probabilidade: 0.009506636299192905\n",
      "Classe prevista: banjo, Probabilidade: 0.00094648014055565\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(top_idxs[0])):\n",
    "    print(f\"Classe prevista: {labels[top_idxs[0][i]]}, Probabilidade: {top_probs[0][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d1b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#luminosidade media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "c92423f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = Image.open(diretorio_imagens + 'ml.jpeg') #alta luminosidade\n",
    "imagem = transform(imagem)\n",
    "x = imagem.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "657634cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = modelo(x)\n",
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_probs, top_idxs = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "16b6938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "79db84fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe prevista: electric guitar, Probabilidade: 0.9990718364715576\n",
      "Classe prevista: acoustic guitar, Probabilidade: 0.000622667430434376\n",
      "Classe prevista: stage, Probabilidade: 6.362604472087696e-05\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(top_idxs[0])):\n",
    "    print(f\"Classe prevista: {labels[top_idxs[0][i]]}, Probabilidade: {top_probs[0][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "73e075f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#luminosidade baixa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "46cf2304",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = Image.open(diretorio_imagens + 'bl.jpeg') #alta luminosidade\n",
    "imagem = transform(imagem)\n",
    "x = imagem.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "308b29ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = modelo(x)\n",
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_probs, top_idxs = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "4ee5bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "0ab6dc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe prevista: electric guitar, Probabilidade: 0.9961141347885132\n",
      "Classe prevista: acoustic guitar, Probabilidade: 0.0036907154135406017\n",
      "Classe prevista: stage, Probabilidade: 4.570334203890525e-05\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(top_idxs[0])):\n",
    "    print(f\"Classe prevista: {labels[top_idxs[0][i]]}, Probabilidade: {top_probs[0][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "8bca5317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classe orang "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "4f7d25db",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = Image.open(diretorio_imagens + 'Orang.jpg') \n",
    "imagem = transform(imagem)\n",
    "x = imagem.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "db88ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = modelo(x)\n",
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_probs, top_idxs = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "7336d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "194172b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe prevista: orangutan, Probabilidade: 0.9997368454933167\n",
      "Classe prevista: siamang, Probabilidade: 0.0001348949153907597\n",
      "Classe prevista: gibbon, Probabilidade: 8.800393698038533e-05\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(top_idxs[0])):\n",
    "    print(f\"Classe prevista: {labels[top_idxs[0][i]]}, Probabilidade: {top_probs[0][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "e0373198",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = Image.open(diretorio_imagens + 'kimono.jpg') \n",
    "imagem = transform(imagem)\n",
    "x = imagem.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "ff74a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = modelo(x)\n",
    "probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "top_probs, top_idxs = torch.topk(probs, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "9c4a8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "94870f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe prevista: suit, Probabilidade: 0.5718713998794556\n",
      "Classe prevista: jean, Probabilidade: 0.08040453493595123\n",
      "Classe prevista: bow tie, Probabilidade: 0.057683419436216354\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(top_idxs[0])):\n",
    "    print(f\"Classe prevista: {labels[top_idxs[0][i]]}, Probabilidade: {top_probs[0][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cfcbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tabela"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47117650",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title></title>\n",
    "</head>\n",
    "<body>\n",
    "    <img src=\"https://cdn.discordapp.com/attachments/1122973016666673224/1125451939296071680/modelocomparacao.png\" style=\"width:700;height:560px\">\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37148cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
